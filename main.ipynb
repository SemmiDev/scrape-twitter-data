{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# https://www.youtube.com/watch?v=jtIMnmbnOFo\n",
    "\n",
    "query = \"(from:elonmusk) until:2023-03-03 since:2023-01-01\"\n",
    "tweets = []\n",
    "limit = 50\n",
    "\n",
    "for tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
    "    if len(tweets) == limit:\n",
    "        break\n",
    "    else:\n",
    "        tweets.append([\n",
    "            # tweet.date,\n",
    "            # tweet.user.username,\n",
    "            tweet.rawContent,\n",
    "        ])\n",
    "\n",
    "# df = pd.DataFrame(tweets, columns=['Date', 'User', 'Tweet'])\n",
    "df = pd.DataFrame(tweets, columns=['Tweet'])\n",
    "\n",
    "# replace \"[^a-zA-Z0-9]\", \" \"\n",
    "df['Tweet'] = df['Tweet'].str.replace(\"[^a-zA-Z0-9]\", \" \")\n",
    "df.to_csv('elonmusk_tweets.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "\n",
    "keyword = \"bbm\"\n",
    "num_of_tweets = 20\n",
    "\n",
    "# Buat list kosong untuk menyimpan hasil scrape\n",
    "tweets_list = []\n",
    "\n",
    "# Scrape data\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper(keyword + ' lang:id').get_items()):\n",
    "    if i>num_of_tweets:\n",
    "        break\n",
    "    tweets_list.append([\n",
    "        tweet.date,\n",
    "        tweet.id,\n",
    "        tweet.user.username,\n",
    "        tweet.likeCount,\n",
    "        tweet.retweetCount,\n",
    "        tweet.rawContent,\n",
    "    ])\n",
    "\n",
    "# Buat dataframe\n",
    "tweets_df = pd.DataFrame(tweets_list, columns=[\n",
    "    'Datetime', 'Tweet Id', 'Username', 'Likes', 'Retweets', 'Text'\n",
    "])\n",
    "\n",
    "# Tampilkan 5 data teratas\n",
    "tweets_df.head()\n",
    "\n",
    "# save\n",
    "tweets_df.to_csv('bbm_tweets.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\sammi-\n",
      "[nltk_data]     vian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\sammi-\n",
      "[nltk_data]     vian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import string\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from IPython.display import display, clear_output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "0     Mau dapet komisi, buat tambahan uang jajan, bu...\n1     dongcaste Mau tampil keren dgn style anime kam...\n2     Website Profesional dengan Harga Terjangkau di...\n3     Pernah kena jahil temen dulu pas SMP dia pasan...\n4     @horatio9797 @ZULFSW @misterjunker @punyae_cin...\n5     @Pencari_Rezeki @amandasah__ @KPK_RI @DivHumas...\n6     Pengamat K3: Buffer Zone terminal BBM lindungi...\n7                                      @draaken bbm pak\n8     Sobat Jatim, sudah tahu belum apa aja sih jeni...\n9     @thedobbybackup2 @ShivSquad1 Fkt voting mhnje ...\n10    Dirut Pertamina Ungkap Hasil Investigasi Kebak...\n11    @kozirama Mungkin krn frekwensi sdh tinggi, di...\n12    @winnershiv1 @ShivSquad1 Ha barobar bhava kon ...\n13    @ReDFloW9002 @DimasSnjy78 @LivingCh @Heraloebs...\n14    tapi, sekolah online bikin gua ngehargain mome...\n15    Jasa iklan bbm ke 50.000 kontak bb bergaransi ...\n16    @Thalomoan1 Apa aja di demo...ðŸ˜„ðŸ˜„\\nMulai dr har...\n17    Bos Pertamina: Simpan 15% Stok BBM Nasional, D...\n18    @nandyays Halo kak,ijin promo ya, Kalau ke Lom...\n19    â€œPenerimaan negara dari pajak kita harapkan bi...\n20    Ion_Bobeica Mau tampil keren dgn style anime k...\nName: Text, dtype: object"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawdf = pd.read_csv('bbm_tweets.csv')\n",
    "rawdf.head()\n",
    "\n",
    "# get tweet text\n",
    "tweet_text = rawdf['Text']\n",
    "tweet_text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case Folding Result : \n",
      "\n",
      "0    mau dapet komisi, buat tambahan uang jajan, bu...\n",
      "1    dongcaste mau tampil keren dgn style anime kam...\n",
      "2    website profesional dengan harga terjangkau di...\n",
      "3    pernah kena jahil temen dulu pas smp dia pasan...\n",
      "4    @horatio9797 @zulfsw @misterjunker @punyae_cin...\n",
      "Name: Text, dtype: object\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------ Case Folding --------\n",
    "# gunakan fungsi Series.str.lower() pada Pandas\n",
    "\n",
    "tweet_text = tweet_text.str.lower()\n",
    "print('Case Folding Result : \\n')\n",
    "print(tweet_text.head(5))\n",
    "print('\\n\\n\\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "import string\n",
    "import re #regex library\n",
    "\n",
    "# import word_tokenize & FreqDist from NLTK\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing Result : \n",
      "\n",
      "0    [mau, dapet, komisi, buat, tambahan, uang, jaj...\n",
      "1    [dongcaste, mau, tampil, keren, dgn, style, an...\n",
      "2    [website, profesional, dengan, harga, terjangk...\n",
      "3    [pernah, kena, jahil, temen, dulu, pas, smp, d...\n",
      "4              [cinta, dibakar, juga, butuh, bbm, mas]\n",
      "Name: token, dtype: object\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------ Tokenizing ---------\n",
    "\n",
    "def remove_tweet_special(text):\n",
    "    # remove tab, new line, ans back slice\n",
    "    text = text.replace('\\\\t',\" \").replace('\\\\n',\" \").replace('\\\\u',\" \").replace('\\\\',\"\")\n",
    "    # remove non ASCII (emoticon, chinese word, .etc)\n",
    "    text = text.encode('ascii', 'replace').decode('ascii')\n",
    "    # remove mention, link, hashtag\n",
    "    text = ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\",\" \", text).split())\n",
    "    # remove incomplete URL\n",
    "    return text.replace(\"http://\", \" \").replace(\"https://\", \" \")\n",
    "\n",
    "tweet_text = tweet_text.apply(remove_tweet_special)\n",
    "\n",
    "#remove number\n",
    "def remove_number(text):\n",
    "    return  re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "tweet_text = tweet_text.apply(remove_number)\n",
    "\n",
    "#remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "\n",
    "tweet_text = tweet_text.apply(remove_punctuation)\n",
    "\n",
    "#remove whitespace leading & trailing\n",
    "def remove_whitespace_LT(text):\n",
    "    return text.strip()\n",
    "\n",
    "tweet_text = tweet_text.apply(remove_whitespace_LT)\n",
    "\n",
    "#remove multiple whitespace into single whitespace\n",
    "def remove_whitespace_multiple(text):\n",
    "    return re.sub('\\s+',' ',text)\n",
    "\n",
    "tweet_text = tweet_text.apply(remove_whitespace_multiple)\n",
    "\n",
    "# remove single char\n",
    "def remove_singl_char(text):\n",
    "    return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "\n",
    "tweet_text = tweet_text.apply(remove_singl_char)\n",
    "\n",
    "# NLTK word rokenize\n",
    "def word_tokenize_wrapper(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "rawdf['token'] = tweet_text.apply(word_tokenize_wrapper)\n",
    "\n",
    "print('Tokenizing Result : \\n')\n",
    "print(rawdf['token'].head(5))\n",
    "print('\\n\\n\\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [dapet, komisi, tambahan, uang, jajan, buruan,...\n",
      "1    [dongcaste, tampil, keren, style, anime, follo...\n",
      "2    [website, profesional, harga, terjangkau, tung...\n",
      "3    [kena, jahil, temen, pas, smp, pasang, status,...\n",
      "4                    [cinta, dibakar, butuh, bbm, mas]\n",
      "Name: tweet_tokens_WSW, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# ----------------------- get stopword from NLTK stopword -------------------------------\n",
    "# get stopword indonesia\n",
    "list_stopwords = stopwords.words('indonesian')\n",
    "\n",
    "# ---------------------------- manualy add stopword  ------------------------------------\n",
    "# append additional stopword\n",
    "list_stopwords.extend([\"yg\", \"dg\", \"rt\", \"dgn\", \"ny\", \"d\", 'klo',\n",
    "                       'kalo', 'amp', 'biar', 'bikin', 'bilang',\n",
    "                       'gak', 'ga', 'krn', 'nya', 'nih', 'sih',\n",
    "                       'si', 'tau', 'tdk', 'tuh', 'utk', 'ya',\n",
    "                       'jd', 'jgn', 'sdh', 'aja', 'n', 't',\n",
    "                       'nyg', 'hehe', 'pen', 'u', 'nan', 'loh', 'rt',\n",
    "                       '&amp', 'yah'])\n",
    "\n",
    "# convert list to dictionary\n",
    "list_stopwords = set(list_stopwords)\n",
    "\n",
    "#remove stopword pada list token\n",
    "def stopwords_removal(words):\n",
    "    return [word for word in words if word not in list_stopwords]\n",
    "\n",
    "rawdf['tweet_tokens_WSW'] = rawdf['token'].apply(stopwords_removal)\n",
    "\n",
    "print(rawdf['tweet_tokens_WSW'].head(5))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n",
      "------------------------\n",
      "dapet : dapet\n",
      "komisi : komisi\n",
      "tambahan : tambah\n",
      "uang : uang\n",
      "jajan : jajan\n",
      "buruan : buru\n",
      "bro : bro\n",
      "daftar : daftar\n",
      "reseller : reseller\n",
      "menguntungkan : untung\n",
      "info : info\n",
      "bbm : bbm\n",
      "bda : bda\n",
      "dongcaste : dongcaste\n",
      "tampil : tampil\n",
      "keren : keren\n",
      "style : style\n",
      "anime : anime\n",
      "follow : follow\n",
      "distroartonline : distroartonline\n",
      "sms : sms\n",
      "bbmbaf : bbmbaf\n",
      "website : website\n",
      "profesional : profesional\n",
      "harga : harga\n",
      "terjangkau : jangkau\n",
      "tunggu : tunggu\n",
      "minat : minat\n",
      "hub : hub\n",
      "fbabb : fbabb\n",
      "kena : kena\n",
      "jahil : jahil\n",
      "temen : temen\n",
      "pas : pas\n",
      "smp : smp\n",
      "pasang : pasang\n",
      "status : status\n",
      "nama : nama\n",
      "ni : ni\n",
      "cowok : cowok\n",
      "pikir : pikir\n",
      "pacar : pacar\n",
      "barunya : baru\n",
      "cengin : cengin\n",
      "dianya : dia\n",
      "senyum : senyum\n",
      "gitu : gitu\n",
      "percaya : percaya\n",
      "ehhh : ehhh\n",
      "taunya : tau\n",
      "cinta : cinta\n",
      "dibakar : bakar\n",
      "butuh : butuh\n",
      "mas : mas\n",
      "rezeki : rezeki\n",
      "ri : ri\n",
      "polri : polri\n",
      "prestasi : prestasi\n",
      "ahok : ahok\n",
      "pertamina : pertamina\n",
      "untung : untung\n",
      "depo : depo\n",
      "terbakar : bakar\n",
      "muraaahhh : muraaahhh\n",
      "banget : banget\n",
      "jargontaik : jargontaik\n",
      "konon : konon\n",
      "pengamat : amat\n",
      "buffer : buffer\n",
      "zone : zone\n",
      "terminal : terminal\n",
      "lindungi : lindung\n",
      "masyarakat : masyarakat\n",
      "bahaya : bahaya\n",
      "beritaja : beritaja\n",
      "sobat : sobat\n",
      "jatim : jatim\n",
      "jenisjenis : jenisjenis\n",
      "tata : tata\n",
      "terkini : kini\n",
      "subsidi : subsidi\n",
      "my : my\n",
      "yuk : yuk\n",
      "simak : simak\n",
      "selengkapnya : lengkap\n",
      "infografik : infografik\n",
      "fkt : fkt\n",
      "voting : voting\n",
      "mhnje : mhnje\n",
      "support : support\n",
      "nasto : nasto\n",
      "khara : khara\n",
      "tya : tya\n",
      "nanatr : nanatr\n",
      "chalu : chalu\n",
      "hoto : hoto\n",
      "he : he\n",
      "kalat : kalat\n",
      "nahi : nahi\n",
      "yana : yana\n",
      "mhnaun : mhnaun\n",
      "gayab : gayab\n",
      "hotata : hotata\n",
      "same : same\n",
      "nantr : nantr\n",
      "zala : zala\n",
      "ani : ani\n",
      "tyacha : tyacha\n",
      "hype : hype\n",
      "gela : gela\n",
      "aata : aata\n",
      "pn : pn\n",
      "aahet : aahet\n",
      "yetat : yetat\n",
      "te : te\n",
      "ase : ase\n",
      "fd : fd\n",
      "division : division\n",
      "che : che\n",
      "tweet : tweet\n",
      "gheun : gheun\n",
      "roj : roj\n",
      "bolayla : bolayla\n",
      "bhetayla : bhetayla\n",
      "shiv : shiv\n",
      "pahije : pahije\n",
      "dirut : dirut\n",
      "hasil : hasil\n",
      "investigasi : investigasi\n",
      "kebakaran : bakar\n",
      "plumpang : plumpang\n",
      "tangki : tangki\n",
      "frekwensi : frekwensi\n",
      "dipakai : pakai\n",
      "ka : ka\n",
      "barang : barang\n",
      "ha : ha\n",
      "barobar : barobar\n",
      "bhava : bhava\n",
      "kon : kon\n",
      "tar : tar\n",
      "ahe : ahe\n",
      "samjhun : samjhun\n",
      "ghenar : ghenar\n",
      "fact : fact\n",
      "mi : mi\n",
      "kel : kel\n",
      "evadh : evadh\n",
      "sangayach : sangayach\n",
      "lokana : lokana\n",
      "koni : koni\n",
      "vicharal : vicharal\n",
      "pan : pan\n",
      "hi : hi\n",
      "gauri : gauri\n",
      "shivsquad : shivsquad\n",
      "member : member\n",
      "instagram : instagram\n",
      "ek : ek\n",
      "post : post\n",
      "takli : takli\n",
      "selfless : selfless\n",
      "kay : kay\n",
      "bolu : bolu\n",
      "dada : dada\n",
      "yanchymule : yanchymule\n",
      "nanter : nanter\n",
      "kam : kam\n",
      "milal : milal\n",
      "nasel : nasel\n",
      "abang : abang\n",
      "warga : warga\n",
      "tangerang : tangerang\n",
      "berita : berita\n",
      "jni : jni\n",
      "mobil : mobil\n",
      "ambulan : ambul\n",
      "isi : isi\n",
      "kesalahan : salah\n",
      "terbesar : besar\n",
      "pegawai : pegawai\n",
      "spbu : spbu\n",
      "supir : supir\n",
      "selesai : selesai\n",
      "lakukan : laku\n",
      "transaksi : transaksi\n",
      "covid : covid\n",
      "emang : emang\n",
      "mobilitas : mobilitas\n",
      "sekolah : sekolah\n",
      "online : online\n",
      "gua : gua\n",
      "ngehargain : ngehargain\n",
      "momen : momen\n",
      "dokumentasiin : dokumentasiin\n",
      "romanticize : romanticize\n",
      "apapun : apa\n",
      "keulang : ulang\n",
      "jasa : jasa\n",
      "iklan : iklan\n",
      "kontak : kontak\n",
      "bb : bb\n",
      "bergaransi : garansi\n",
      "rb : rb\n",
      "dijamin : jamin\n",
      "pinc : pinc\n",
      "iklanklaten : iklanklaten\n",
      "twittiklan : twittiklan\n",
      "demo : demo\n",
      "dr : dr\n",
      "gas : gas\n",
      "vaksin : vaksin\n",
      "ciptaker : ciptaker\n",
      "omnibus : omnibus\n",
      "bos : bos\n",
      "simpan : simpan\n",
      "stok : stok\n",
      "nasional : nasional\n",
      "langsung : langsung\n",
      "pindah : pindah\n",
      "halo : halo\n",
      "kakijin : kakijin\n",
      "promo : promo\n",
      "lombok : lombok\n",
      "hubungi : hubung\n",
      "kak : kak\n",
      "sewa : sewa\n",
      "avanza : avanza\n",
      "full : full\n",
      "day : day\n",
      "include : include\n",
      "driver : driver\n",
      "wa : wa\n",
      "terima : terima\n",
      "kasih : kasih\n",
      "penerimaan : terima\n",
      "negara : negara\n",
      "pajak : pajak\n",
      "harapkan : harap\n",
      "pakai : pakai\n",
      "listrik : listrik\n",
      "pupuk : pupuk\n",
      "dana : dana\n",
      "desa : desa\n",
      "bantuan : bantu\n",
      "sosial : sosial\n",
      "membangun : bangun\n",
      "jalan : jalan\n",
      "pelabuhan : labuh\n",
      "ionbobeica : ionbobeica\n",
      "{'dapet': 'dapet', 'komisi': 'komisi', 'tambahan': 'tambah', 'uang': 'uang', 'jajan': 'jajan', 'buruan': 'buru', 'bro': 'bro', 'daftar': 'daftar', 'reseller': 'reseller', 'menguntungkan': 'untung', 'info': 'info', 'bbm': 'bbm', 'bda': 'bda', 'dongcaste': 'dongcaste', 'tampil': 'tampil', 'keren': 'keren', 'style': 'style', 'anime': 'anime', 'follow': 'follow', 'distroartonline': 'distroartonline', 'sms': 'sms', 'bbmbaf': 'bbmbaf', 'website': 'website', 'profesional': 'profesional', 'harga': 'harga', 'terjangkau': 'jangkau', 'tunggu': 'tunggu', 'minat': 'minat', 'hub': 'hub', 'fbabb': 'fbabb', 'kena': 'kena', 'jahil': 'jahil', 'temen': 'temen', 'pas': 'pas', 'smp': 'smp', 'pasang': 'pasang', 'status': 'status', 'nama': 'nama', 'ni': 'ni', 'cowok': 'cowok', 'pikir': 'pikir', 'pacar': 'pacar', 'barunya': 'baru', 'cengin': 'cengin', 'dianya': 'dia', 'senyum': 'senyum', 'gitu': 'gitu', 'percaya': 'percaya', 'ehhh': 'ehhh', 'taunya': 'tau', 'cinta': 'cinta', 'dibakar': 'bakar', 'butuh': 'butuh', 'mas': 'mas', 'rezeki': 'rezeki', 'ri': 'ri', 'polri': 'polri', 'prestasi': 'prestasi', 'ahok': 'ahok', 'pertamina': 'pertamina', 'untung': 'untung', 'depo': 'depo', 'terbakar': 'bakar', 'muraaahhh': 'muraaahhh', 'banget': 'banget', 'jargontaik': 'jargontaik', 'konon': 'konon', 'pengamat': 'amat', 'buffer': 'buffer', 'zone': 'zone', 'terminal': 'terminal', 'lindungi': 'lindung', 'masyarakat': 'masyarakat', 'bahaya': 'bahaya', 'beritaja': 'beritaja', 'sobat': 'sobat', 'jatim': 'jatim', 'jenisjenis': 'jenisjenis', 'tata': 'tata', 'terkini': 'kini', 'subsidi': 'subsidi', 'my': 'my', 'yuk': 'yuk', 'simak': 'simak', 'selengkapnya': 'lengkap', 'infografik': 'infografik', 'fkt': 'fkt', 'voting': 'voting', 'mhnje': 'mhnje', 'support': 'support', 'nasto': 'nasto', 'khara': 'khara', 'tya': 'tya', 'nanatr': 'nanatr', 'chalu': 'chalu', 'hoto': 'hoto', 'he': 'he', 'kalat': 'kalat', 'nahi': 'nahi', 'yana': 'yana', 'mhnaun': 'mhnaun', 'gayab': 'gayab', 'hotata': 'hotata', 'same': 'same', 'nantr': 'nantr', 'zala': 'zala', 'ani': 'ani', 'tyacha': 'tyacha', 'hype': 'hype', 'gela': 'gela', 'aata': 'aata', 'pn': 'pn', 'aahet': 'aahet', 'yetat': 'yetat', 'te': 'te', 'ase': 'ase', 'fd': 'fd', 'division': 'division', 'che': 'che', 'tweet': 'tweet', 'gheun': 'gheun', 'roj': 'roj', 'bolayla': 'bolayla', 'bhetayla': 'bhetayla', 'shiv': 'shiv', 'pahije': 'pahije', 'dirut': 'dirut', 'hasil': 'hasil', 'investigasi': 'investigasi', 'kebakaran': 'bakar', 'plumpang': 'plumpang', 'tangki': 'tangki', 'frekwensi': 'frekwensi', 'dipakai': 'pakai', 'ka': 'ka', 'barang': 'barang', 'ha': 'ha', 'barobar': 'barobar', 'bhava': 'bhava', 'kon': 'kon', 'tar': 'tar', 'ahe': 'ahe', 'samjhun': 'samjhun', 'ghenar': 'ghenar', 'fact': 'fact', 'mi': 'mi', 'kel': 'kel', 'evadh': 'evadh', 'sangayach': 'sangayach', 'lokana': 'lokana', 'koni': 'koni', 'vicharal': 'vicharal', 'pan': 'pan', 'hi': 'hi', 'gauri': 'gauri', 'shivsquad': 'shivsquad', 'member': 'member', 'instagram': 'instagram', 'ek': 'ek', 'post': 'post', 'takli': 'takli', 'selfless': 'selfless', 'kay': 'kay', 'bolu': 'bolu', 'dada': 'dada', 'yanchymule': 'yanchymule', 'nanter': 'nanter', 'kam': 'kam', 'milal': 'milal', 'nasel': 'nasel', 'abang': 'abang', 'warga': 'warga', 'tangerang': 'tangerang', 'berita': 'berita', 'jni': 'jni', 'mobil': 'mobil', 'ambulan': 'ambul', 'isi': 'isi', 'kesalahan': 'salah', 'terbesar': 'besar', 'pegawai': 'pegawai', 'spbu': 'spbu', 'supir': 'supir', 'selesai': 'selesai', 'lakukan': 'laku', 'transaksi': 'transaksi', 'covid': 'covid', 'emang': 'emang', 'mobilitas': 'mobilitas', 'sekolah': 'sekolah', 'online': 'online', 'gua': 'gua', 'ngehargain': 'ngehargain', 'momen': 'momen', 'dokumentasiin': 'dokumentasiin', 'romanticize': 'romanticize', 'apapun': 'apa', 'keulang': 'ulang', 'jasa': 'jasa', 'iklan': 'iklan', 'kontak': 'kontak', 'bb': 'bb', 'bergaransi': 'garansi', 'rb': 'rb', 'dijamin': 'jamin', 'pinc': 'pinc', 'iklanklaten': 'iklanklaten', 'twittiklan': 'twittiklan', 'demo': 'demo', 'dr': 'dr', 'gas': 'gas', 'vaksin': 'vaksin', 'ciptaker': 'ciptaker', 'omnibus': 'omnibus', 'bos': 'bos', 'simpan': 'simpan', 'stok': 'stok', 'nasional': 'nasional', 'langsung': 'langsung', 'pindah': 'pindah', 'halo': 'halo', 'kakijin': 'kakijin', 'promo': 'promo', 'lombok': 'lombok', 'hubungi': 'hubung', 'kak': 'kak', 'sewa': 'sewa', 'avanza': 'avanza', 'full': 'full', 'day': 'day', 'include': 'include', 'driver': 'driver', 'wa': 'wa', 'terima': 'terima', 'kasih': 'kasih', 'penerimaan': 'terima', 'negara': 'negara', 'pajak': 'pajak', 'harapkan': 'harap', 'pakai': 'pakai', 'listrik': 'listrik', 'pupuk': 'pupuk', 'dana': 'dana', 'desa': 'desa', 'bantuan': 'bantu', 'sosial': 'sosial', 'membangun': 'bangun', 'jalan': 'jalan', 'pelabuhan': 'labuh', 'ionbobeica': 'ionbobeica'}\n",
      "------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": "Pandas Apply:   0%|          | 0/21 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "93f175ff559848eab3b7d89cf1186a13"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     [dapet, komisi, tambah, uang, jajan, buru, bro...\n",
      "1     [dongcaste, tampil, keren, style, anime, follo...\n",
      "2     [website, profesional, harga, jangkau, tunggu,...\n",
      "3     [kena, jahil, temen, pas, smp, pasang, status,...\n",
      "4                       [cinta, bakar, butuh, bbm, mas]\n",
      "5     [rezeki, ri, polri, prestasi, ahok, pertamina,...\n",
      "6     [amat, buffer, zone, terminal, bbm, lindung, m...\n",
      "7                                                 [bbm]\n",
      "8     [sobat, jatim, jenisjenis, bbm, tata, kini, da...\n",
      "9     [fkt, voting, mhnje, support, nasto, khara, su...\n",
      "10    [dirut, pertamina, hasil, investigasi, bakar, ...\n",
      "11                  [frekwensi, pakai, ka, barang, bbm]\n",
      "12    [ha, barobar, bhava, kon, tar, ahe, samjhun, g...\n",
      "13    [abang, warga, tangerang, berita, jni, mobil, ...\n",
      "14    [sekolah, online, gua, ngehargain, momen, doku...\n",
      "15    [jasa, iklan, bbm, kontak, bb, garansi, rb, ja...\n",
      "16    [demo, dr, harga, gas, harga, bbm, vaksin, cip...\n",
      "17    [bos, pertamina, simpan, stok, bbm, nasional, ...\n",
      "18    [halo, kakijin, promo, lombok, hubung, kak, se...\n",
      "19    [terima, negara, pajak, harap, pakai, subsidi,...\n",
      "20    [ionbobeica, tampil, keren, style, anime, foll...\n",
      "Name: tweet_tokens_stemmed, dtype: object\n"
     ]
    }
   ],
   "source": [
    "### --- Stemming --- ####\n",
    "\n",
    "# import Sastrawi package\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import swifter\n",
    "\n",
    "# create stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# stemmed\n",
    "def stemmed_wrapper(term):\n",
    "    return stemmer.stem(term)\n",
    "\n",
    "term_dict = {}\n",
    "\n",
    "for document in rawdf['tweet_tokens_WSW']:\n",
    "    for term in document:\n",
    "        if term not in term_dict:\n",
    "            term_dict[term] = ' '\n",
    "\n",
    "print(len(term_dict))\n",
    "print(\"------------------------\")\n",
    "\n",
    "for term in term_dict:\n",
    "    term_dict[term] = stemmed_wrapper(term)\n",
    "    print(term,\":\" ,term_dict[term])\n",
    "\n",
    "print(term_dict)\n",
    "print(\"------------------------\")\n",
    "\n",
    "\n",
    "# apply stemmed term to dataframe\n",
    "def get_stemmed_term(document):\n",
    "    return [term_dict[term] for term in document]\n",
    "\n",
    "rawdf['tweet_tokens_stemmed'] = rawdf['tweet_tokens_WSW'].swifter.apply(get_stemmed_term)\n",
    "print(rawdf['tweet_tokens_stemmed'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "rawdf.to_csv(\"preprocessing_bbm.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mau dapet komisi buat tambah uang jajan buru bro daftar jadi reseller kami untung info bbm bda\n",
      "dongcaste mau tampil keren dgn style anime kamu follow distroartonline sms bbmbaf\n",
      "website profesional dengan harga jangkau di mulai dari tunggu apa lagi minat hub bbm di fbabb\n",
      "pernah kena jahil temen dulu pas smp dia pasang status nama ni cowok di bbm aku pikir itu pacar baru kan jadi aku cengin dia cuma senyum gitu jadi aku percaya ehhh tau\n",
      "cinta bakar juga butuh bbm mas\n",
      "rezeki ri polri prestasi ahok pertamina untung besar tidak pernah yg nama nya depo bakar harga bbm muraaahhh banget jargontaik konon kata\n",
      "amat buffer zone terminal bbm lindung masyarakat dari bahaya beritaja\n",
      "bbm pak\n",
      "sobat jatim sudah tahu belum apa aja sih jenisjenis bbm atau tata cara kini daftar subsidi di my pertamina yuk simak lengkap di infografik ikut\n",
      "fkt voting mhnje support nasto khara support tya nanatr chalu hoto he kalat nahi yana mhnaun gayab hotata same bbm nantr zala ani tyacha hype gela aata pn gayab aahet ani yetat te he ase fd division che tweet gheun fkt roj bolayla bhetayla shiv pahije yana\n",
      "dirut pertamina ungkap hasil investigasi bakar depo bbm plumpang bukan dari tangki\n",
      "mungkin krn frekwensi sdh tinggi pakai utk ka barang amp bbm juga\n",
      "ha barobar bhava kon tar ahe samjhun ghenar yana fact mi he kel mi te kel evadh sangayach ahe lokana koni vicharal pan nahi ani hi shiv gauri shivsquad member instagram ek post tar takli ahe ani selfless support kay bolu dada yanchymule bbm nanter kam milal nasel\n",
      "kalo abang warga tangerang pasti tahu berita ini nah jni mobil ambul yang lagi isi bbm salah besar ya ada di pegawai spbu karena supir saya sudah selesai laku transaksi saat itu covid emang lagi tinggi jadi mobilitas ambul juga tinggi\n",
      "tapi sekolah online bikin gua ngehargain momen yang ada sekarang semua hal di dokumentasiin di romanticize se kecil apa karena momen itu belum tentu akan ulang lagi\n",
      "jasa iklan bbm ke kontak bb garansi hanya rb jamin info pinc iklanklaten twittiklan\n",
      "apa aja di demo mulai dr harga gas harga bbm vaksin ciptaker omnibus\n",
      "bos pertamina simpan stok bbm nasional depo plumpang tidak bisa langsung pindah\n",
      "halo kakijin promo ya kalau ke lombok hubung kami ya kak sewa mobil avanza full day include driver dan bbm bisa wa ke terima kasih\n",
      "terima negara dari pajak kita harap bisa nanti kita pakai untuk subsidi bbm subsidi listrik subsidi pupuk untuk dana desa untuk bantu sosial untuk bangun jalan untuk bangun labuh\n",
      "ionbobeica mau tampil keren dgn style anime kamu follow distroartonline sms bbmbaf\n"
     ]
    }
   ],
   "source": [
    "# import StemmerFactory class\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "# create stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "for i in tweet_text:\n",
    "  # stemming process\n",
    "  sentence = i\n",
    "  output   = stemmer.stem(sentence)\n",
    "\n",
    "  print(output)\n",
    "  # ekonomi indonesia sedang dalam tumbuh yang bangga\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "                                               tweet\n0  ['dapet', 'komisi', 'tambah', 'uang', 'jajan',...\n1  ['dongcaste', 'tampil', 'keren', 'style', 'ani...\n2  ['website', 'profesional', 'harga', 'jangkau',...\n3  ['kena', 'jahil', 'temen', 'pas', 'smp', 'pasa...\n4          ['cinta', 'bakar', 'butuh', 'bbm', 'mas']",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>['dapet', 'komisi', 'tambah', 'uang', 'jajan',...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>['dongcaste', 'tampil', 'keren', 'style', 'ani...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>['website', 'profesional', 'harga', 'jangkau',...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>['kena', 'jahil', 'temen', 'pas', 'smp', 'pasa...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>['cinta', 'bakar', 'butuh', 'bbm', 'mas']</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "TWEET_DATA = pd.read_csv(\"preprocessing_bbm.csv\", usecols=[\"tweet_tokens_stemmed\"])\n",
    "TWEET_DATA.columns = [\"tweet\"]\n",
    "\n",
    "TWEET_DATA.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "                                               tweet  \\\n0  ['dapet', 'komisi', 'tambah', 'uang', 'jajan',...   \n1  ['dongcaste', 'tampil', 'keren', 'style', 'ani...   \n2  ['website', 'profesional', 'harga', 'jangkau',...   \n3  ['kena', 'jahil', 'temen', 'pas', 'smp', 'pasa...   \n4          ['cinta', 'bakar', 'butuh', 'bbm', 'mas']   \n\n                                          tweet_list  \n0  [dapet, komisi, tambah, uang, jajan, buru, bro...  \n1  [dongcaste, tampil, keren, style, anime, follo...  \n2  [website, profesional, harga, jangkau, tunggu,...  \n3  [kena, jahil, temen, pas, smp, pasang, status,...  \n4                    [cinta, bakar, butuh, bbm, mas]  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n      <th>tweet_list</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>['dapet', 'komisi', 'tambah', 'uang', 'jajan',...</td>\n      <td>[dapet, komisi, tambah, uang, jajan, buru, bro...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>['dongcaste', 'tampil', 'keren', 'style', 'ani...</td>\n      <td>[dongcaste, tampil, keren, style, anime, follo...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>['website', 'profesional', 'harga', 'jangkau',...</td>\n      <td>[website, profesional, harga, jangkau, tunggu,...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>['kena', 'jahil', 'temen', 'pas', 'smp', 'pasa...</td>\n      <td>[kena, jahil, temen, pas, smp, pasang, status,...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>['cinta', 'bakar', 'butuh', 'bbm', 'mas']</td>\n      <td>[cinta, bakar, butuh, bbm, mas]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert list formated string to list\n",
    "import ast\n",
    "\n",
    "def convert_text_list(texts):\n",
    "    texts = ast.literal_eval(texts)\n",
    "    return [text for text in texts]\n",
    "\n",
    "TWEET_DATA[\"tweet_list\"] = TWEET_DATA[\"tweet\"].apply(convert_text_list)\n",
    "TWEET_DATA.head()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "0    {'dapet': 0.07692307692307693, 'komisi': 0.076...\n1    {'dongcaste': 0.1111111111111111, 'tampil': 0....\n2    {'website': 0.1111111111111111, 'profesional':...\n3    {'kena': 0.047619047619047616, 'jahil': 0.0476...\n4    {'cinta': 0.2, 'bakar': 0.2, 'butuh': 0.2, 'bb...\nName: TF_dict, dtype: object"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_TF(document):\n",
    "    # Counts the number of times the word appears in review\n",
    "    TF_dict = {}\n",
    "    for term in document:\n",
    "        if term in TF_dict:\n",
    "            TF_dict[term] += 1\n",
    "        else:\n",
    "            TF_dict[term] = 1\n",
    "    # Computes tf for each word\n",
    "    for term in TF_dict:\n",
    "        # rumus = frekuensi kemunculan term / jumlah term pada dokumen\n",
    "        TF_dict[term] = TF_dict[term] / len(document)\n",
    "    return TF_dict\n",
    "\n",
    "TWEET_DATA[\"TF_dict\"] = TWEET_DATA['tweet_list'].apply(calc_TF)\n",
    "TWEET_DATA[\"TF_dict\"].head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                term \t TF\n",
      "\n",
      "              rezeki \t 0.0625\n",
      "                  ri \t 0.0625\n",
      "               polri \t 0.0625\n",
      "            prestasi \t 0.0625\n",
      "                ahok \t 0.0625\n",
      "           pertamina \t 0.0625\n",
      "              untung \t 0.0625\n",
      "                nama \t 0.0625\n",
      "                depo \t 0.0625\n",
      "               bakar \t 0.0625\n",
      "               harga \t 0.0625\n",
      "                 bbm \t 0.0625\n",
      "           muraaahhh \t 0.0625\n",
      "              banget \t 0.0625\n",
      "          jargontaik \t 0.0625\n",
      "               konon \t 0.0625\n"
     ]
    }
   ],
   "source": [
    "index = 5\n",
    "\n",
    "print('%20s' % \"term\", \"\\t\", \"TF\\n\")\n",
    "for key in TWEET_DATA[\"TF_dict\"][index]:\n",
    "    print('%20s' % key, \"\\t\", TWEET_DATA[\"TF_dict\"][index][key])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "def calc_DF(tfDict):\n",
    "    count_DF = {}\n",
    "    # Run through each document's tf dictionary and increment countDict's (term, doc) pair\n",
    "    for document in tfDict:\n",
    "        for term in document:\n",
    "            if term in count_DF:\n",
    "                count_DF[term] += 1\n",
    "            else:\n",
    "                count_DF[term] = 1\n",
    "    return count_DF\n",
    "\n",
    "DF = calc_DF(TWEET_DATA[\"TF_dict\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "n_document = len(TWEET_DATA)\n",
    "\n",
    "def calc_IDF(__n_document, __DF):\n",
    "    IDF_Dict = {}\n",
    "    for term in __DF:\n",
    "        IDF_Dict[term] = np.log(__n_document / (__DF[term] + 1))\n",
    "    return IDF_Dict\n",
    "\n",
    "#Stores the idf dictionary\n",
    "IDF = calc_IDF(n_document, DF)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "#calc TF-IDF\n",
    "def calc_TF_IDF(TF):\n",
    "    TF_IDF_Dict = {}\n",
    "    #For each word in the review, we multiply its tf and its idf.\n",
    "    for key in TF:\n",
    "        TF_IDF_Dict[key] = TF[key] * IDF[key]\n",
    "    return TF_IDF_Dict\n",
    "\n",
    "#Stores the TF-IDF Series\n",
    "TWEET_DATA[\"TF-IDF_dict\"] = TWEET_DATA[\"TF_dict\"].apply(calc_TF_IDF)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                term \t         TF \t              TF-IDF\n",
      "\n",
      "               dirut \t 0.1111111111111111 \t 0.26126391746260863\n",
      "           pertamina \t 0.1111111111111111 \t 0.15945383614325806\n",
      "               hasil \t 0.1111111111111111 \t 0.26126391746260863\n",
      "         investigasi \t 0.1111111111111111 \t 0.26126391746260863\n",
      "               bakar \t 0.1111111111111111 \t 0.18424756406705914\n",
      "                depo \t 0.1111111111111111 \t 0.18424756406705914\n",
      "                 bbm \t 0.1111111111111111 \t 0.01112038428410918\n",
      "            plumpang \t 0.1111111111111111 \t 0.21621223878392368\n",
      "              tangki \t 0.1111111111111111 \t 0.26126391746260863\n"
     ]
    }
   ],
   "source": [
    "# Check TF-IDF result\n",
    "index = 10\n",
    "\n",
    "print('%20s' % \"term\", \"\\t\", '%10s' % \"TF\", \"\\t\", '%20s' % \"TF-IDF\\n\")\n",
    "for key in TWEET_DATA[\"TF-IDF_dict\"][index]:\n",
    "    print('%20s' % key, \"\\t\", TWEET_DATA[\"TF_dict\"][index][key] ,\"\\t\" , TWEET_DATA[\"TF-IDF_dict\"][index][key])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print first row matrix TF_IDF_Vec Series\n",
      "\n",
      "[0.007698727581306356, 0.0, 0.0, 0.0, 0.0, 0.14968539608117795, 0.14968539608117795, 0.14968539608117795, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18087501978180598, 0.18087501978180598, 0.18087501978180598, 0.18087501978180598, 0.18087501978180598, 0.18087501978180598, 0.18087501978180598, 0.18087501978180598, 0.18087501978180598, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "matrix size :  50\n"
     ]
    }
   ],
   "source": [
    "# sort descending by value for DF dictionary\n",
    "sorted_DF = sorted(DF.items(), key=lambda kv: kv[1], reverse=True)[:50]\n",
    "\n",
    "# Create a list of unique words from sorted dictionay `sorted_DF`\n",
    "unique_term = [item[0] for item in sorted_DF]\n",
    "\n",
    "def calc_TF_IDF_Vec(__TF_IDF_Dict):\n",
    "    TF_IDF_vector = [0.0] * len(unique_term)\n",
    "\n",
    "    # For each unique word, if it is in the review, store its TF-IDF value.\n",
    "    for i, term in enumerate(unique_term):\n",
    "        if term in __TF_IDF_Dict:\n",
    "            TF_IDF_vector[i] = __TF_IDF_Dict[term]\n",
    "    return TF_IDF_vector\n",
    "\n",
    "TWEET_DATA[\"TF_IDF_Vec\"] = TWEET_DATA[\"TF-IDF_dict\"].apply(calc_TF_IDF_Vec)\n",
    "\n",
    "print(\"print first row matrix TF_IDF_Vec Series\\n\")\n",
    "print(TWEET_DATA[\"TF_IDF_Vec\"][0])\n",
    "\n",
    "print(\"\\nmatrix size : \", len(TWEET_DATA[\"TF_IDF_Vec\"][0]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "               term      rank\n2             harga  0.656382\n3             bakar  0.619532\n1         pertamina  0.495161\n26            pakai  0.491598\n4              depo  0.453710\n17          subsidi  0.446243\n11            anime  0.432424\n15           bbmbaf  0.432424\n14              sms  0.432424\n12           follow  0.432424\n13  distroartonline  0.432424\n10            style  0.432424\n9             keren  0.432424\n8            tampil  0.432424\n25         plumpang  0.410803\n7              info  0.311845\n5            daftar  0.288679\n6            untung  0.271305\n45            fbabb  0.261264\n44              hub  0.261264\n43            minat  0.261264\n42           tunggu  0.261264\n41          jangkau  0.261264\n40      profesional  0.261264\n39          website  0.261264\n38        dongcaste  0.261264\n0               bbm  0.248951\n28           terima  0.216882\n16             nama  0.214282\n27            mobil  0.207128\n30           komisi  0.180875\n31           tambah  0.180875\n32             uang  0.180875\n33            jajan  0.180875\n34             buru  0.180875\n35              bro  0.180875\n37              bda  0.180875\n29            dapet  0.180875\n36         reseller  0.180875\n22              ani  0.163884\n21             yana  0.123344\n19               he  0.123344\n18          support  0.123344\n46             kena  0.111970\n48            temen  0.111970\n47            jahil  0.111970\n49              pas  0.111970\n20             nahi  0.081942\n23               te  0.081942\n24             shiv  0.081942",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>term</th>\n      <th>rank</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>harga</td>\n      <td>0.656382</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>bakar</td>\n      <td>0.619532</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>pertamina</td>\n      <td>0.495161</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>pakai</td>\n      <td>0.491598</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>depo</td>\n      <td>0.453710</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>subsidi</td>\n      <td>0.446243</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>anime</td>\n      <td>0.432424</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>bbmbaf</td>\n      <td>0.432424</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>sms</td>\n      <td>0.432424</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>follow</td>\n      <td>0.432424</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>distroartonline</td>\n      <td>0.432424</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>style</td>\n      <td>0.432424</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>keren</td>\n      <td>0.432424</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>tampil</td>\n      <td>0.432424</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>plumpang</td>\n      <td>0.410803</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>info</td>\n      <td>0.311845</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>daftar</td>\n      <td>0.288679</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>untung</td>\n      <td>0.271305</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>fbabb</td>\n      <td>0.261264</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>hub</td>\n      <td>0.261264</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>minat</td>\n      <td>0.261264</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>tunggu</td>\n      <td>0.261264</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>jangkau</td>\n      <td>0.261264</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>profesional</td>\n      <td>0.261264</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>website</td>\n      <td>0.261264</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>dongcaste</td>\n      <td>0.261264</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>bbm</td>\n      <td>0.248951</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>terima</td>\n      <td>0.216882</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>nama</td>\n      <td>0.214282</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>mobil</td>\n      <td>0.207128</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>komisi</td>\n      <td>0.180875</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>tambah</td>\n      <td>0.180875</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>uang</td>\n      <td>0.180875</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>jajan</td>\n      <td>0.180875</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>buru</td>\n      <td>0.180875</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>bro</td>\n      <td>0.180875</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>bda</td>\n      <td>0.180875</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>dapet</td>\n      <td>0.180875</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>reseller</td>\n      <td>0.180875</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>ani</td>\n      <td>0.163884</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>yana</td>\n      <td>0.123344</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>he</td>\n      <td>0.123344</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>support</td>\n      <td>0.123344</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>kena</td>\n      <td>0.111970</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>temen</td>\n      <td>0.111970</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>jahil</td>\n      <td>0.111970</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>pas</td>\n      <td>0.111970</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>nahi</td>\n      <td>0.081942</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>te</td>\n      <td>0.081942</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>shiv</td>\n      <td>0.081942</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert Series to List\n",
    "TF_IDF_Vec_List = np.array(TWEET_DATA[\"TF_IDF_Vec\"].to_list())\n",
    "\n",
    "# Sum element vector in axis=0\n",
    "sums = TF_IDF_Vec_List.sum(axis=0)\n",
    "\n",
    "data = []\n",
    "\n",
    "for col, term in enumerate(unique_term):\n",
    "    data.append((term, sums[col]))\n",
    "\n",
    "ranking = pd.DataFrame(data, columns=['term', 'rank'])\n",
    "ranking.sort_values('rank', ascending=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "ranking.to_csv(\"bbm_ranking.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
